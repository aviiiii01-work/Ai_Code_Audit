{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14494891,"sourceType":"datasetVersion","datasetId":9257942}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes transformers peft accelerate\n!pip install -q -U gguf \n!pip install -q -U sentencepiece ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T12:44:03.013554Z","iopub.execute_input":"2026-01-14T12:44:03.013906Z","iopub.status.idle":"2026-01-14T12:44:31.288327Z","shell.execute_reply.started":"2026-01-14T12:44:03.013872Z","shell.execute_reply":"2026-01-14T12:44:31.287351Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport os\n\nbase_model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nadapter_path = \"/kaggle/input/adapters/adapters\" \nmerged_dir = \"./merged_model\"\n\nprint(f\" Checking for adapters in: {adapter_path} \")\nif os.path.exists(adapter_path):\n    print(\"Found folder! Contents:\", os.listdir(adapter_path))\nelse:\n    print(f\" Error: Could not find folder at {adapter_path}\")\n    print(\"Please check the 'Input' sidebar in Kaggle and copy the path exactly.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T12:55:54.854756Z","iopub.execute_input":"2026-01-14T12:55:54.855293Z","iopub.status.idle":"2026-01-14T12:55:54.888370Z","shell.execute_reply.started":"2026-01-14T12:55:54.855264Z","shell.execute_reply":"2026-01-14T12:55:54.887621Z"}},"outputs":[{"name":"stdout","text":"--- Checking for adapters in: /kaggle/input/adapters/adapters ---\nFound folder! Contents: ['adapter_model.safetensors', 'adapter_config.json', 'README.md']\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"print(\" Loading Base Model (FP16) \")\ntokenizer = AutoTokenizer.from_pretrained(base_model_path)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_path, \n    torch_dtype=torch.float16, \n    device_map=\"cpu\" \n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T12:56:43.837114Z","iopub.execute_input":"2026-01-14T12:56:43.837453Z","iopub.status.idle":"2026-01-14T12:56:45.993106Z","shell.execute_reply.started":"2026-01-14T12:56:43.837424Z","shell.execute_reply":"2026-01-14T12:56:45.992327Z"}},"outputs":[{"name":"stdout","text":" Loading Base Model (FP16) \n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"try:\n    print(\"--- Merging LoRA Adapters into Base ---\")\n    model = PeftModel.from_pretrained(base_model, adapter_path)\n    merged_model = model.merge_and_unload()\n\n    merged_model.save_pretrained(merged_dir)\n    tokenizer.save_pretrained(merged_dir)\n    print(f\" SUCCESS: Merged model saved to {merged_dir}\")\nexcept Exception as e:\n    print(f\" MERGE ERROR: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T12:57:05.611067Z","iopub.execute_input":"2026-01-14T12:57:05.611773Z","iopub.status.idle":"2026-01-14T12:57:15.904500Z","shell.execute_reply.started":"2026-01-14T12:57:05.611743Z","shell.execute_reply":"2026-01-14T12:57:15.903719Z"}},"outputs":[{"name":"stdout","text":"--- Merging LoRA Adapters into Base ---\n SUCCESS: Merged model saved to ./merged_model\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\nimport shutil\n\nmerged_dir = \"./merged_model\"\noutput_base = \"./quantized\"\n\nprint(\" Starting INT8 Quantization \")\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n    merged_dir,\n    load_in_8bit=True,\n    device_map=\"auto\"\n)\nmodel_8bit.save_pretrained(f\"{output_base}/model-int8\")\nprint(\"INT8 model saved to ./quantized/model-int8\")\n\ndel model_8bit\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T13:04:38.337750Z","iopub.execute_input":"2026-01-14T13:04:38.338532Z","iopub.status.idle":"2026-01-14T13:04:41.471773Z","shell.execute_reply.started":"2026-01-14T13:04:38.338494Z","shell.execute_reply":"2026-01-14T13:04:41.470843Z"}},"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"name":"stdout","text":" Starting INT8 Quantization \nINT8 model saved to ./quantized/model-int8\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"print(\"\\n Starting INT4 (NF4) Quantization \")\nnf4_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True\n)\n\nmodel_4bit = AutoModelForCausalLM.from_pretrained(\n    merged_dir,\n    quantization_config=nf4_config,\n    device_map=\"auto\"\n)\nmodel_4bit.save_pretrained(f\"{output_base}/model-int4\")\nprint(\" INT4 model saved to ./quantized/model-int4\")\n\ntokenizer = AutoTokenizer.from_pretrained(merged_dir)\ntokenizer.save_pretrained(f\"{output_base}/model-int8\")\ntokenizer.save_pretrained(f\"{output_base}/model-int4\")\n\nprint(\"\\n Summary of Files Created \")\n!du -sh ./quantized/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T13:05:01.013761Z","iopub.execute_input":"2026-01-14T13:05:01.014060Z","iopub.status.idle":"2026-01-14T13:05:04.370932Z","shell.execute_reply.started":"2026-01-14T13:05:01.014033Z","shell.execute_reply":"2026-01-14T13:05:04.370197Z"}},"outputs":[{"name":"stdout","text":"\n Starting INT4 (NF4) Quantization \n INT4 model saved to ./quantized/model-int4\n\n Summary of Files Created \n732M\t./quantized/model-int4\n1.2G\t./quantized/model-int8\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"print(\"--- Building llama.cpp with CMake ---\")\n!cd llama.cpp && mkdir -p build && cd build && cmake .. && cmake --build . --config Release -j\n\nquantize_path = \"./llama.cpp/build/bin/llama-quantize\"\nif os.path.exists(quantize_path):\n    print(f\" Found quantizer at {quantize_path}\")\nelse:\n    quantize_path = \"./llama.cpp/build/llama-quantize\"\n    print(f\"Checking fallback path: {quantize_path}\")\n\nprint(\"\\n--- Step A: Converting to GGUF (FP16) ---\")\n!python llama.cpp/convert_hf_to_gguf.py ./merged_model \\\n    --outfile ./quantized/model.fp16.gguf \\\n    --outtype f16\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T13:12:21.326323Z","iopub.execute_input":"2026-01-14T13:12:21.327142Z","iopub.status.idle":"2026-01-14T13:18:50.355717Z","shell.execute_reply.started":"2026-01-14T13:12:21.327105Z","shell.execute_reply":"2026-01-14T13:18:50.354655Z"}},"outputs":[{"name":"stdout","text":"--- Building llama.cpp with CMake ---\n-- The C compiler identification is GNU 11.4.0\n-- The CXX compiler identification is GNU 11.4.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n-- Found Git: /usr/bin/git (found version \"2.34.1\")\n-- The ASM compiler identification is GNU\n-- Found assembler: /usr/bin/cc\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n-- Found Threads: TRUE\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\n-- GGML_SYSTEM_ARCH: x86\n-- Including CPU backend\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n-- Found OpenMP: TRUE (found version \"4.5\")\n-- x86 detected\n-- Adding CPU backend variant ggml-cpu: -march=native \n-- ggml version: 0.9.5\n-- ggml commit:  3e4bb29\n-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n-- Downloading tinyllamas/stories15M-q4_0.gguf from ggml-org/models...\n-- Generating embedded license file for target: common\n-- Configuring done (3.6s)\n-- Generating done (0.3s)\n-- Build files have been written to: /kaggle/working/llama.cpp/build\n[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n[  0%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n[  1%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n[  1%] \u001b[32mBuilding CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\u001b[0m\n[  2%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n[  2%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n[  4%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n[  5%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n[  5%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n[  5%] Built target build_info\n[  5%] Built target sha1\n[  5%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n[  5%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n[  5%] Built target sha256\n[  6%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n[  6%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n[  6%] Built target llama-gemma3-cli\n[  6%] Built target llama-minicpmv-cli\n[  6%] Built target llama-qwen2vl-cli\n[  6%] Built target llama-llava-cli\n[  6%] Built target xxhash\n[  6%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n[  6%] Built target ggml-base\n[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n[  8%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n[ 10%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n[ 10%] Built target ggml-cpu\n[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n[ 11%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n[ 11%] Built target ggml\n[ 11%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n[ 11%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\u001b[0m\n[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\u001b[0m\n[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\u001b[0m\n[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\u001b[0m\n[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\u001b[0m\n[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\u001b[0m\n[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\u001b[0m\n[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\u001b[0m\n[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\u001b[0m\n[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\u001b[0m\n[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\u001b[0m\n[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\u001b[0m\n[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\u001b[0m\n[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\u001b[0m\n[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\u001b[0m\n[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\u001b[0m\n[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\u001b[0m\n[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\u001b[0m\n[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\u001b[0m\n[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\u001b[0m\n[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\u001b[0m\n[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\u001b[0m\n[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\u001b[0m\n[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\u001b[0m\n[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\u001b[0m\n[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\u001b[0m\n[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\u001b[0m\n[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o\u001b[0m\n[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\u001b[0m\n[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\u001b[0m\n[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\u001b[0m\n[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\u001b[0m\n[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\u001b[0m\n[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\u001b[0m\n[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o\u001b[0m\n[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\u001b[0m\n[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\u001b[0m\n[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\u001b[0m\n[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\u001b[0m\n[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\u001b[0m\n[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\u001b[0m\n[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\u001b[0m\n[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\u001b[0m\n[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\u001b[0m\n[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\u001b[0m\n[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\u001b[0m\n[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\u001b[0m\n[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\u001b[0m\n[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\u001b[0m\n[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\u001b[0m\n[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\u001b[0m\n[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\u001b[0m\n[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\u001b[0m\n[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/maincoder.cpp.o\u001b[0m\n[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\u001b[0m\n[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o\u001b[0m\n[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\u001b[0m\n[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\u001b[0m\n[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/modern-bert.cpp.o\u001b[0m\n[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\u001b[0m\n[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\u001b[0m\n[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\u001b[0m\n[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\u001b[0m\n[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\u001b[0m\n[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\u001b[0m\n[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\u001b[0m\n[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\u001b[0m\n[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\u001b[0m\n[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\u001b[0m\n[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\u001b[0m\n[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\u001b[0m\n[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\u001b[0m\n[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\u001b[0m\n[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\u001b[0m\n[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo3.cpp.o\u001b[0m\n[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\u001b[0m\n[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\u001b[0m\n[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\u001b[0m\n[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\u001b[0m\n[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\u001b[0m\n[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\u001b[0m\n[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\u001b[0m\n[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\u001b[0m\n[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\u001b[0m\n[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\u001b[0m\n[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\u001b[0m\n[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\u001b[0m\n[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\u001b[0m\n[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\u001b[0m\n[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\u001b[0m\n[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\u001b[0m\n[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\u001b[0m\n[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\u001b[0m\n[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\u001b[0m\n[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\u001b[0m\n[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\u001b[0m\n[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\u001b[0m\n[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\u001b[0m\n[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\u001b[0m\n[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\u001b[0m\n[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\u001b[0m\n[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\u001b[0m\n[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\u001b[0m\n[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\u001b[0m\n[ 44%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n[ 44%] Built target llama-gguf\n[ 44%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n[ 44%] Built target llama-gguf-hash\n[ 44%] \u001b[32m\u001b[1mLinking CXX static library libcpp-httplib.a\u001b[0m\n[ 44%] Built target cpp-httplib\n[ 44%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n[ 44%] Built target llama\n[ 44%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n[ 45%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n[ 45%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n[ 46%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n[ 47%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n[ 47%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n[ 47%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n[ 47%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\u001b[0m\n[ 48%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\u001b[0m\n[ 48%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n[ 49%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/cogvlm.cpp.o\u001b[0m\n[ 49%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/conformer.cpp.o\u001b[0m\n[ 49%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n[ 49%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n[ 49%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n[ 49%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n[ 50%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/download.cpp.o\u001b[0m\n[ 50%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/glm4v.cpp.o\u001b[0m\n[ 51%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/kimivl.cpp.o\u001b[0m\n[ 51%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/internvl.cpp.o\u001b[0m\n[ 51%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n[ 51%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n[ 51%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n[ 52%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n[ 52%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llava.cpp.o\u001b[0m\n[ 52%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llama4.cpp.o\u001b[0m\n[ 52%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n[ 52%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/minicpmv.cpp.o\u001b[0m\n[ 53%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/pixtral.cpp.o\u001b[0m\n[ 53%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen2vl.cpp.o\u001b[0m\n[ 53%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\u001b[0m\n[ 53%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen3vl.cpp.o\u001b[0m\n[ 54%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n[ 54%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/preset.cpp.o\u001b[0m\n[ 54%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/siglip.cpp.o\u001b[0m\n[ 54%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/whisper-enc.cpp.o\u001b[0m\n[ 54%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n[ 55%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/mobilenetv5.cpp.o\u001b[0m\n[ 55%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/youtuvl.cpp.o\u001b[0m\n[ 55%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n[ 55%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/unicode.cpp.o\u001b[0m\n[ 56%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/__/license.cpp.o\u001b[0m\n[ 56%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n[ 56%] Built target test-c\n[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n[ 56%] Built target llama-simple\n[ 56%] Built target llama-simple-chat\n[ 56%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n[ 56%] Built target mtmd\n[ 56%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n[ 56%] Built target common\n[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/test-chat-peg-parser.cpp.o\u001b[0m\n[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/test-peg-parser.cpp.o\u001b[0m\n[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/get-model.cpp.o\u001b[0m\n[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-basic.cpp.o\u001b[0m\n[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-gbnf-generation.cpp.o\u001b[0m\n[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-parser.cpp.o\u001b[0m\n[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-serialization.cpp.o\u001b[0m\n[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-unicode.cpp.o\u001b[0m\n[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/test-state-restore-fragmented.cpp.o\u001b[0m\n[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/get-model.cpp.o\u001b[0m\n[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-sampler.dir/test-backend-sampler.cpp.o\u001b[0m\n[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-sampler.dir/get-model.cpp.o\u001b[0m\n[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/get-model.cpp.o\u001b[0m\n[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n[ 73%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n[ 73%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/test-alloc.cpp.o\u001b[0m\n[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/get-model.cpp.o\u001b[0m\n[ 73%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n[ 73%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n[ 73%] \u001b[32mBuilding CXX object examples/debug/CMakeFiles/llama-debug.dir/debug.cpp.o\u001b[0m\n[ 73%] \u001b[32mBuilding CXX object examples/idle/CMakeFiles/llama-idle.dir/idle.cpp.o\u001b[0m\n[ 73%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n[ 73%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n[ 73%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n[ 73%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n[ 73%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n[ 74%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n[ 75%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n[ 75%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n[ 75%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n[ 75%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n[ 76%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n[ 76%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n[ 76%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n[ 76%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n[ 76%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n[ 76%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n[ 76%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n[ 76%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n[ 76%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n[ 76%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n[ 76%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n[ 76%] \u001b[32mBuilding CXX object tools/completion/CMakeFiles/llama-completion.dir/completion.cpp.o\u001b[0m\n[ 77%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n[ 78%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n[ 78%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-task.cpp.o\u001b[0m\n[ 78%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n[ 79%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n[ 80%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-queue.cpp.o\u001b[0m\n[ 80%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n[ 80%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-common.cpp.o\u001b[0m\n[ 80%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n[ 80%] \u001b[32mBuilding CXX object tools/fit-params/CMakeFiles/llama-fit-params.dir/fit-params.cpp.o\u001b[0m\n[ 80%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-context.cpp.o\u001b[0m\n[ 80%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n[ 80%] Built target test-mtmd-c-api\n[ 80%] Built target test-model-load-cancel\n[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n[ 82%] Built target test-log\n[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n[ 82%] Built target test-rope\n[ 82%] Built target test-autorelease\n[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n[ 82%] Built target test-quantize-fns\n[ 82%] Built target test-barrier\n[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n[ 82%] Built target llama-lookup-merge\n[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n[ 82%] Built target llama-q8dot\n[ 82%] Built target test-tokenizer-1-spm\n[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n[ 83%] Built target test-tokenizer-1-bpe\n[ 83%] Built target llama-tokenize\n[ 83%] Built target llama-vdot\n[ 83%] Built target test-gbnf-validator\n[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-alloc\u001b[0m\n[ 84%] Built target test-alloc\n[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-state-restore-fragmented\u001b[0m\n[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-fit-params\u001b[0m\n[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n[ 86%] Built target llama-gguf-split\n[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-idle\u001b[0m\n[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n[ 88%] Built target test-tokenizer-0\n[ 88%] Built target test-grammar-parser\n[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n[ 89%] Built target test-sampling\n[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n[ 90%] Built target test-state-restore-fragmented\n[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n[ 90%] Built target llama-fit-params\n[ 90%] Built target test-regex-partial\n[ 90%] Built target llama-lookup-create\n[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n[ 90%] Built target test-quantize-perf\n[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n[ 90%] Built target test-llama-grammar\n[ 90%] Built target llama-finetune\n[ 90%] Built target llama-idle\n[ 90%] Built target llama-save-load-state\n[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n[ 90%] Built target llama-speculative-simple\n[ 90%] Built target llama-eval-callback\n[ 90%] Built target llama-lookup-stats\n[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n[ 91%] Built target llama-batched\n[ 91%] Built target llama-lookup\n[ 91%] Built target llama-batched-bench\n[ 91%] Built target test-thread-safety\n[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n[ 91%] Built target test-opt\n[ 91%] Built target llama-gen-docs\n[ 91%] Built target llama-convert-llama2c-to-ggml\n[ 91%] Built target llama-passkey\n[ 91%] Built target test-arg-parser\n[ 91%] Built target llama-lookahead\n[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n[ 91%] Built target llama-embedding\n[ 91%] Built target test-gguf\n[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n[ 91%] Built target llama-parallel\n[ 91%] Built target llama-retrieval\n[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n[ 91%] Built target llama-export-lora\n[ 91%] Built target llama-quantize\n[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n[ 92%] Built target llama-mtmd-cli\n[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-sampler\u001b[0m\n[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n[ 94%] Built target test-backend-sampler\n[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n[ 94%] Built target test-chat-template\n[ 94%] Built target llama-cvector-generator\n[ 94%] Built target llama-speculative\n[ 94%] Built target test-json-partial\n[ 94%] Built target llama-diffusion-cli\n[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-completion\u001b[0m\n[ 95%] Built target llama-completion\n[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n[ 95%] Built target test-quantize-stats\n[ 95%] Built target llama-perplexity\n[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-debug\u001b[0m\n[ 95%] Built target llama-debug\n[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n[ 95%] Built target test-grammar-integration\n[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-peg-parser\u001b[0m\n[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n[ 95%] Built target test-peg-parser\n[ 95%] Built target test-chat-parser\n[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n[ 96%] Built target test-json-schema-to-grammar\n[ 96%] Built target llama-imatrix\n[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n[ 97%] Built target llama-bench\n[ 97%] Built target llama-tts\n[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-peg-parser\u001b[0m\n[ 97%] Built target test-chat-peg-parser\n[ 97%] \u001b[32m\u001b[1mLinking CXX static library libserver-context.a\u001b[0m\n[ 97%] Built target server-context\n[ 97%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n[ 98%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n[ 98%] \u001b[32mBuilding CXX object tools/cli/CMakeFiles/llama-cli.dir/cli.cpp.o\u001b[0m\n[ 98%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\u001b[0m\n[ 98%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\u001b[0m\n[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n[ 99%] Built target test-backend-ops\n[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n[ 99%] Built target llama-cli\n[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n[ 99%] Built target llama-server\n[100%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n[100%] Built target test-chat\n Found quantizer at ./llama.cpp/build/bin/llama-quantize\n\n--- Step A: Converting to GGUF (FP16) ---\nINFO:hf-to-gguf:Loading model: merged_model\nINFO:hf-to-gguf:Model architecture: LlamaForCausalLM\nINFO:hf-to-gguf:gguf: indexing model part 'model.safetensors'\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Exporting model...\nINFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {2048, 32000}\nINFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {2048, 32000}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\nINFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\nINFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {2048}\nINFO:hf-to-gguf:Set meta model\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 2048\nINFO:hf-to-gguf:gguf: embedding length = 2048\nINFO:hf-to-gguf:gguf: feed forward length = 5632\nINFO:hf-to-gguf:gguf: head count = 32\nINFO:hf-to-gguf:gguf: key-value head count = 4\nINFO:hf-to-gguf:gguf: rope theta = 10000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model quantization version\nINFO:hf-to-gguf:Set model tokenizer\nWARNING:gguf.vocab:Unknown separator token '<s>' in TemplateProcessing<pair>\nINFO:gguf.vocab:Setting special token type bos to 1\nINFO:gguf.vocab:Setting special token type eos to 2\nINFO:gguf.vocab:Setting special token type unk to 0\nINFO:gguf.vocab:Setting special token type pad to 2\nINFO:gguf.vocab:Setting add_bos_token to True\nINFO:gguf.vocab:Setting add_sep_token to False\nINFO:gguf.vocab:Setting add_eos_token to False\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}\n{% if message['role'] == 'user' %}\n{{ '<|user|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'system' %}\n{{ '<|system|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'assistant' %}\n{{ '<|assistant|>\n'  + message['content'] + eos_token }}\n{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ '<|assistant|>' }}\n{% endif %}\n{% endfor %}\nINFO:gguf.gguf_writer:Writing the following files:\nINFO:gguf.gguf_writer:quantized/model.fp16.gguf: n_tensors = 201, total_size = 2.2G\nWriting: 100%|██████████████████████████| 2.20G/2.20G [00:01<00:00, 1.55Gbyte/s]\nINFO:hf-to-gguf:Model successfully exported to quantized/model.fp16.gguf\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"\nprint(\"\\n Step B: Quantizing GGUF to Q4_0 \")\n!{quantize_path} ./quantized/model.fp16.gguf ./quantized/model.gguf q4_0\n\nprint(\"\\n Step C: Quantizing GGUF to Q8_0 \")\n!{quantize_path} ./quantized/model.fp16.gguf ./quantized/model.gguf_q8_0 q8_0\n\nif os.path.exists(\"./quantized/model.fp16.gguf\"):\n    os.remove(\"./quantized/model.fp16.gguf\")\n\nprint(\"\\n Final GGUF files in ./quantized/:\")\n!ls -lh ./quantized/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T13:20:00.686222Z","iopub.execute_input":"2026-01-14T13:20:00.686637Z","iopub.status.idle":"2026-01-14T13:20:25.693751Z","shell.execute_reply.started":"2026-01-14T13:20:00.686602Z","shell.execute_reply":"2026-01-14T13:20:25.693000Z"}},"outputs":[{"name":"stdout","text":"\n Step B: Quantizing GGUF to Q4_0 \nmain: build = 1 (3e4bb29)\nmain: built with GNU 11.4.0 for Linux x86_64\nmain: quantizing './quantized/model.fp16.gguf' to './quantized/model.gguf' as Q4_0\nllama_model_loader: direct I/O is enabled, disabling mmap\nllama_model_loader: loaded meta data with 32 key-value pairs and 201 tensors from ./quantized/model.fp16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Merged_Model\nllama_model_loader: - kv   3:                         general.size_label str              = 1.1B\nllama_model_loader: - kv   4:                          llama.block_count u32              = 22\nllama_model_loader: - kv   5:                       llama.context_length u32              = 2048\nllama_model_loader: - kv   6:                     llama.embedding_length u32              = 2048\nllama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 5632\nllama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  12:                 llama.attention.key_length u32              = 64\nllama_model_loader: - kv  13:               llama.attention.value_length u32              = 64\nllama_model_loader: - kv  14:                          general.file_type u32              = 1\nllama_model_loader: - kv  15:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 64\nllama_model_loader: - kv  17:               general.quantization_version u32              = 2\nllama_model_loader: - kv  18:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  21:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  25:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 2\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_sep_token bool             = false\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\nllama_model_loader: - kv  31:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - type  f32:   45 tensors\nllama_model_loader: - type  f16:  156 tensors\n[   1/ 201]                        output.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q6_K .. size =   125.00 MiB ->    51.27 MiB\n[   2/ 201]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[   3/ 201]                    token_embd.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q4_0 .. size =   125.00 MiB ->    35.16 MiB\n[   4/ 201]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[   5/ 201]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[   6/ 201]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[   7/ 201]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[   8/ 201]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[   9/ 201]                blk.0.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  10/ 201]                blk.0.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  11/ 201]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  12/ 201]                  blk.0.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  13/ 201]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  14/ 201]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  15/ 201]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  16/ 201]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  17/ 201]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  18/ 201]                blk.1.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  19/ 201]                blk.1.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  20/ 201]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  21/ 201]                  blk.1.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  22/ 201]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  23/ 201]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  24/ 201]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  25/ 201]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  26/ 201]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  27/ 201]                blk.2.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  28/ 201]                blk.2.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  29/ 201]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  30/ 201]                  blk.2.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  31/ 201]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  32/ 201]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  33/ 201]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  34/ 201]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  35/ 201]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  36/ 201]                blk.3.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  37/ 201]                blk.3.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  38/ 201]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  39/ 201]                  blk.3.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  40/ 201]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  41/ 201]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  42/ 201]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  43/ 201]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  44/ 201]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  45/ 201]                blk.4.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  46/ 201]                blk.4.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  47/ 201]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  48/ 201]                  blk.4.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  49/ 201]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  50/ 201]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  51/ 201]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  52/ 201]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  53/ 201]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  54/ 201]                blk.5.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  55/ 201]                blk.5.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  56/ 201]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  57/ 201]                  blk.5.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  58/ 201]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  59/ 201]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  60/ 201]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  61/ 201]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  62/ 201]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  63/ 201]                blk.6.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  64/ 201]                blk.6.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  65/ 201]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  66/ 201]                  blk.6.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  67/ 201]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  68/ 201]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  69/ 201]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  70/ 201]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  71/ 201]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  72/ 201]                blk.7.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  73/ 201]                blk.7.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  74/ 201]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  75/ 201]                  blk.7.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  76/ 201]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  77/ 201]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  78/ 201]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  79/ 201]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  80/ 201]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  81/ 201]                blk.8.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  82/ 201]                blk.8.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  83/ 201]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  84/ 201]                  blk.8.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  85/ 201]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  86/ 201]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  87/ 201]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  88/ 201]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  89/ 201]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  90/ 201]                blk.9.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  91/ 201]                blk.9.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  92/ 201]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  93/ 201]                  blk.9.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[  94/ 201]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  95/ 201]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  96/ 201]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  97/ 201]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[  98/ 201]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[  99/ 201]               blk.10.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 100/ 201]               blk.10.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 101/ 201]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 102/ 201]                 blk.10.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 103/ 201]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 104/ 201]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 105/ 201]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 106/ 201]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 107/ 201]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 108/ 201]               blk.11.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 109/ 201]               blk.11.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 110/ 201]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 111/ 201]                 blk.11.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 112/ 201]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 113/ 201]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 114/ 201]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 115/ 201]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 116/ 201]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 117/ 201]               blk.12.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 118/ 201]               blk.12.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 119/ 201]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 120/ 201]                 blk.12.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 121/ 201]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 122/ 201]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 123/ 201]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 124/ 201]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 125/ 201]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 126/ 201]               blk.13.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 127/ 201]               blk.13.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 128/ 201]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 129/ 201]                 blk.13.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 130/ 201]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 131/ 201]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 132/ 201]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 133/ 201]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 134/ 201]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 135/ 201]               blk.14.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 136/ 201]               blk.14.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 137/ 201]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 138/ 201]                 blk.14.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 139/ 201]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 140/ 201]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 141/ 201]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 142/ 201]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 143/ 201]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 144/ 201]               blk.15.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 145/ 201]               blk.15.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 146/ 201]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 147/ 201]                 blk.15.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 148/ 201]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 149/ 201]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 150/ 201]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 151/ 201]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 152/ 201]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 153/ 201]               blk.16.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 154/ 201]               blk.16.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 155/ 201]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 156/ 201]                 blk.16.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 157/ 201]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 158/ 201]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 159/ 201]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 160/ 201]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 161/ 201]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 162/ 201]               blk.17.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 163/ 201]               blk.17.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 164/ 201]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 165/ 201]                 blk.17.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 166/ 201]                 blk.18.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 167/ 201]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 168/ 201]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 169/ 201]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 170/ 201]                 blk.18.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 171/ 201]               blk.18.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 172/ 201]               blk.18.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 173/ 201]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 174/ 201]                 blk.18.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 175/ 201]                 blk.19.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 176/ 201]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 177/ 201]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 178/ 201]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 179/ 201]                 blk.19.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 180/ 201]               blk.19.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 181/ 201]               blk.19.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 182/ 201]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 183/ 201]                 blk.19.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 184/ 201]                 blk.20.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 185/ 201]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 186/ 201]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 187/ 201]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 188/ 201]                 blk.20.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 189/ 201]               blk.20.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 190/ 201]               blk.20.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 191/ 201]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 192/ 201]                 blk.20.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 193/ 201]                 blk.21.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 194/ 201]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 195/ 201]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 196/ 201]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB\n[ 197/ 201]                 blk.21.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_0 .. size =     1.00 MiB ->     0.28 MiB\n[ 198/ 201]               blk.21.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 199/ 201]               blk.21.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\n[ 200/ 201]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 201/ 201]                 blk.21.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_0 .. size =    22.00 MiB ->     6.19 MiB\nllama_model_quantize_impl: model size  =  2098.35 MiB\nllama_model_quantize_impl: quant size  =   606.53 MiB\n\nmain: quantize time = 11405.14 ms\nmain:    total time = 11405.14 ms\n\n Step C: Quantizing GGUF to Q8_0 \nmain: build = 1 (3e4bb29)\nmain: built with GNU 11.4.0 for Linux x86_64\nmain: quantizing './quantized/model.fp16.gguf' to './quantized/model.gguf_q8_0' as Q8_0\nllama_model_loader: direct I/O is enabled, disabling mmap\nllama_model_loader: loaded meta data with 32 key-value pairs and 201 tensors from ./quantized/model.fp16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Merged_Model\nllama_model_loader: - kv   3:                         general.size_label str              = 1.1B\nllama_model_loader: - kv   4:                          llama.block_count u32              = 22\nllama_model_loader: - kv   5:                       llama.context_length u32              = 2048\nllama_model_loader: - kv   6:                     llama.embedding_length u32              = 2048\nllama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 5632\nllama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  12:                 llama.attention.key_length u32              = 64\nllama_model_loader: - kv  13:               llama.attention.value_length u32              = 64\nllama_model_loader: - kv  14:                          general.file_type u32              = 1\nllama_model_loader: - kv  15:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 64\nllama_model_loader: - kv  17:               general.quantization_version u32              = 2\nllama_model_loader: - kv  18:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  21:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  25:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 2\nllama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  28:               tokenizer.ggml.add_sep_token bool             = false\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\nllama_model_loader: - kv  31:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - type  f32:   45 tensors\nllama_model_loader: - type  f16:  156 tensors\n[   1/ 201]                        output.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   125.00 MiB ->    66.41 MiB\n[   2/ 201]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[   3/ 201]                    token_embd.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   125.00 MiB ->    66.41 MiB\n[   4/ 201]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[   5/ 201]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[   6/ 201]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[   7/ 201]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[   8/ 201]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[   9/ 201]                blk.0.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  10/ 201]                blk.0.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  11/ 201]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  12/ 201]                  blk.0.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  13/ 201]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  14/ 201]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  15/ 201]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  16/ 201]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  17/ 201]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  18/ 201]                blk.1.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  19/ 201]                blk.1.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  20/ 201]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  21/ 201]                  blk.1.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  22/ 201]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  23/ 201]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  24/ 201]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  25/ 201]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  26/ 201]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  27/ 201]                blk.2.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  28/ 201]                blk.2.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  29/ 201]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  30/ 201]                  blk.2.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  31/ 201]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  32/ 201]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  33/ 201]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  34/ 201]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  35/ 201]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  36/ 201]                blk.3.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  37/ 201]                blk.3.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  38/ 201]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  39/ 201]                  blk.3.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  40/ 201]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  41/ 201]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  42/ 201]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  43/ 201]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  44/ 201]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  45/ 201]                blk.4.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  46/ 201]                blk.4.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  47/ 201]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  48/ 201]                  blk.4.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  49/ 201]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  50/ 201]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  51/ 201]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  52/ 201]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  53/ 201]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  54/ 201]                blk.5.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  55/ 201]                blk.5.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  56/ 201]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  57/ 201]                  blk.5.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  58/ 201]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  59/ 201]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  60/ 201]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  61/ 201]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  62/ 201]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  63/ 201]                blk.6.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  64/ 201]                blk.6.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  65/ 201]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  66/ 201]                  blk.6.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  67/ 201]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  68/ 201]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  69/ 201]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  70/ 201]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  71/ 201]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  72/ 201]                blk.7.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  73/ 201]                blk.7.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  74/ 201]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  75/ 201]                  blk.7.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  76/ 201]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  77/ 201]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  78/ 201]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  79/ 201]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  80/ 201]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  81/ 201]                blk.8.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  82/ 201]                blk.8.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  83/ 201]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  84/ 201]                  blk.8.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  85/ 201]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  86/ 201]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  87/ 201]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  88/ 201]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  89/ 201]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  90/ 201]                blk.9.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  91/ 201]                blk.9.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  92/ 201]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  93/ 201]                  blk.9.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[  94/ 201]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  95/ 201]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[  96/ 201]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  97/ 201]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[  98/ 201]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[  99/ 201]               blk.10.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 100/ 201]               blk.10.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 101/ 201]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 102/ 201]                 blk.10.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 103/ 201]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 104/ 201]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 105/ 201]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 106/ 201]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 107/ 201]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 108/ 201]               blk.11.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 109/ 201]               blk.11.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 110/ 201]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 111/ 201]                 blk.11.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 112/ 201]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 113/ 201]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 114/ 201]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 115/ 201]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 116/ 201]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 117/ 201]               blk.12.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 118/ 201]               blk.12.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 119/ 201]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 120/ 201]                 blk.12.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 121/ 201]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 122/ 201]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 123/ 201]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 124/ 201]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 125/ 201]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 126/ 201]               blk.13.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 127/ 201]               blk.13.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 128/ 201]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 129/ 201]                 blk.13.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 130/ 201]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 131/ 201]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 132/ 201]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 133/ 201]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 134/ 201]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 135/ 201]               blk.14.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 136/ 201]               blk.14.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 137/ 201]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 138/ 201]                 blk.14.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 139/ 201]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 140/ 201]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 141/ 201]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 142/ 201]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 143/ 201]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 144/ 201]               blk.15.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 145/ 201]               blk.15.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 146/ 201]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 147/ 201]                 blk.15.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 148/ 201]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 149/ 201]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 150/ 201]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 151/ 201]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 152/ 201]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 153/ 201]               blk.16.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 154/ 201]               blk.16.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 155/ 201]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 156/ 201]                 blk.16.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 157/ 201]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 158/ 201]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 159/ 201]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 160/ 201]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 161/ 201]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 162/ 201]               blk.17.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 163/ 201]               blk.17.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 164/ 201]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 165/ 201]                 blk.17.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 166/ 201]                 blk.18.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 167/ 201]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 168/ 201]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 169/ 201]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 170/ 201]                 blk.18.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 171/ 201]               blk.18.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 172/ 201]               blk.18.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 173/ 201]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 174/ 201]                 blk.18.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 175/ 201]                 blk.19.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 176/ 201]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 177/ 201]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 178/ 201]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 179/ 201]                 blk.19.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 180/ 201]               blk.19.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 181/ 201]               blk.19.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 182/ 201]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 183/ 201]                 blk.19.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 184/ 201]                 blk.20.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 185/ 201]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 186/ 201]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 187/ 201]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 188/ 201]                 blk.20.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 189/ 201]               blk.20.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 190/ 201]               blk.20.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 191/ 201]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 192/ 201]                 blk.20.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 193/ 201]                 blk.21.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 194/ 201]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 195/ 201]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 196/ 201]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n[ 197/ 201]                 blk.21.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n[ 198/ 201]               blk.21.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 199/ 201]               blk.21.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n[ 200/ 201]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MiB\n[ 201/ 201]                 blk.21.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\nllama_model_quantize_impl: model size  =  2098.35 MiB\nllama_model_quantize_impl: quant size  =  1114.91 MiB\n\nmain: quantize time = 12134.65 ms\nmain:    total time = 12134.65 ms\n\n Final GGUF files in ./quantized/:\ntotal 1.7G\n-rw-r--r-- 1 root root 608M Jan 14 13:20 model.gguf\n-rw-r--r-- 1 root root 1.1G Jan 14 13:20 model.gguf_q8_0\ndrwxr-xr-x 2 root root 4.0K Jan 14 13:05 model-int4\ndrwxr-xr-x 2 root root 4.0K Jan 14 13:05 model-int8\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import time\n\ndef get_file_size(path):\n    if os.path.isfile(path):\n        return os.path.getsize(path) / (1024 * 1024) \n    elif os.path.isdir(path):\n        return sum(os.path.getsize(os.path.join(dirpath, f)) for dirpath, _, filenames in os.walk(path) for f in filenames) / (1024 * 1024)\n    return 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T13:26:28.494270Z","iopub.execute_input":"2026-01-14T13:26:28.494702Z","iopub.status.idle":"2026-01-14T13:26:28.501754Z","shell.execute_reply.started":"2026-01-14T13:26:28.494665Z","shell.execute_reply":"2026-01-14T13:26:28.501039Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def benchmark_transformers(model_path, is_int4=False, is_int8=False):\n    print(f\" Benchmarking {model_path} \")\n    tokenizer = AutoTokenizer.from_pretrained(\"./merged_model\")\n    \n    if is_int4:\n        from transformers import BitsAndBytesConfig\n        config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n        model = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=config, device_map=\"auto\")\n    elif is_int8:\n        model = AutoModelForCausalLM.from_pretrained(model_path, load_in_8bit=True, device_map=\"auto\")\n    else:\n        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n\n    prompt = \"Explain the importance of model quantization in one sentence.\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    \n    _ = model.generate(**inputs, max_new_tokens=1)\n    \n    start = time.time()\n    outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n    end = time.time()\n    \n    tokens = len(outputs[0]) - len(inputs.input_ids[0])\n    tps = tokens / (end - start)\n    \n    del model\n    torch.cuda.empty_cache()\n    return round(tps, 2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T13:27:59.703850Z","iopub.execute_input":"2026-01-14T13:27:59.704716Z","iopub.status.idle":"2026-01-14T13:27:59.715641Z","shell.execute_reply.started":"2026-01-14T13:27:59.704664Z","shell.execute_reply":"2026-01-14T13:27:59.714768Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"\nresults = {\n    \"FP16\": {\"Size (MB)\": get_file_size(\"./merged_model\")},\n    \"INT8\": {\"Size (MB)\": get_file_size(\"./quantized/model-int8\")},\n    \"INT4\": {\"Size (MB)\": get_file_size(\"./quantized/model-int4\")},\n    \"GGUF\": {\"Size (MB)\": get_file_size(\"./quantized/model.gguf\")},\n}\n\nresults[\"FP16\"][\"Speed (TPS)\"] = benchmark_transformers(\"./merged_model\")\nresults[\"INT8\"][\"Speed (TPS)\"] = benchmark_transformers(\"./quantized/model-int8\", is_int8=True)\nresults[\"INT4\"][\"Speed (TPS)\"] = benchmark_transformers(\"./quantized/model-int4\", is_int4=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T13:50:36.511191Z","iopub.execute_input":"2026-01-14T13:50:36.511561Z","iopub.status.idle":"2026-01-14T13:51:03.636696Z","shell.execute_reply.started":"2026-01-14T13:50:36.511530Z","shell.execute_reply":"2026-01-14T13:51:03.635899Z"}},"outputs":[{"name":"stdout","text":" Benchmarking ./merged_model \n","output_type":"stream"},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"name":"stdout","text":" Benchmarking ./quantized/model-int8 \n Benchmarking ./quantized/model-int4 \n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import os\n\nreport_content = \"\"\"# QUANTISATION-REPORT.md\n\n## Project: TinyLlama Fine-Tuning & Optimization (Day 3)\n\n### 1. Formats Generated\n- **INT8 (8-bit):** Quantized using `bitsandbytes`. Balanced for accuracy and memory.\n- **INT4 (4-bit):** Quantized using `NF4` (NormalFloat4). Maximum compression for GPU.\n- **GGUF (Q4_0):** Converted via `llama.cpp`. Optimized for CPU/Edge inference.\n\n### 2. Deliverables Location\n- `/quantized/model-int8/`\n- `/quantized/model-int4/`\n- `/quantized/model.gguf`\n\n### 3. Summary of Methodology\n1. **Model Merging:** Successfully merged Day 2 LoRA adapters into the FP16 base TinyLlama model.\n2. **Post-Training Quantization:** Applied 8-bit and 4-bit quantization to the weights.\n3. **Format Conversion:** Used `llama.cpp`'s conversion script to move from Safetensors to the GGUF binary format.\n4. **Compression:** Effectively reduced the model footprint from ~2.2GB (FP16) down to ~650MB (INT4).\n\"\"\"\n\nwith open(\"./quantized/QUANTISATION-REPORT.md\", \"w\") as f:\n    f.write(report_content)\n\nprint(\"QUANTISATION-REPORT.md has been created in ./quantized/\")\n\nprint(\"\\n Final Deliverables Checklist \")\n!ls -R ./quantized/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T13:52:43.599852Z","iopub.execute_input":"2026-01-14T13:52:43.600840Z","iopub.status.idle":"2026-01-14T13:52:44.569802Z","shell.execute_reply.started":"2026-01-14T13:52:43.600806Z","shell.execute_reply":"2026-01-14T13:52:44.569056Z"}},"outputs":[{"name":"stdout","text":"QUANTISATION-REPORT.md has been created in ./quantized/\n\n Final Deliverables Checklist \n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"./quantized/:\nmodel.gguf  model.gguf_q8_0  model-int4  model-int8  QUANTISATION-REPORT.md\n\n./quantized/model-int4:\nchat_template.jinja\tmodel.safetensors\t tokenizer.json\nconfig.json\t\tspecial_tokens_map.json  tokenizer.model\ngeneration_config.json\ttokenizer_config.json\n\n./quantized/model-int8:\nchat_template.jinja\tmodel.safetensors\t tokenizer.json\nconfig.json\t\tspecial_tokens_map.json  tokenizer.model\ngeneration_config.json\ttokenizer_config.json\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"!zip -r day3_complete_project.zip ./quantized ./merged_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T13:56:13.031260Z","iopub.execute_input":"2026-01-14T13:56:13.032222Z","iopub.status.idle":"2026-01-14T14:04:19.370295Z","shell.execute_reply.started":"2026-01-14T13:56:13.032183Z","shell.execute_reply":"2026-01-14T14:04:19.369574Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: quantized/ (stored 0%)\n  adding: quantized/model.gguf (deflated 5%)\n  adding: quantized/model.gguf_q8_0 (deflated 4%)\n  adding: quantized/model-int8/ (stored 0%)\n  adding: quantized/model-int8/config.json (deflated 56%)\n  adding: quantized/model-int8/tokenizer_config.json (deflated 69%)\n  adding: quantized/model-int8/tokenizer.json (deflated 85%)\n  adding: quantized/model-int8/special_tokens_map.json (deflated 79%)\n  adding: quantized/model-int8/chat_template.jinja (deflated 60%)\n  adding: quantized/model-int8/tokenizer.model (deflated 55%)\n  adding: quantized/model-int8/generation_config.json (deflated 29%)\n  adding: quantized/model-int8/model.safetensors (deflated 14%)\n  adding: quantized/model-int4/ (stored 0%)\n  adding: quantized/model-int4/config.json (deflated 55%)\n  adding: quantized/model-int4/tokenizer_config.json (deflated 69%)\n  adding: quantized/model-int4/tokenizer.json (deflated 85%)\n  adding: quantized/model-int4/special_tokens_map.json (deflated 79%)\n  adding: quantized/model-int4/chat_template.jinja (deflated 60%)\n  adding: quantized/model-int4/tokenizer.model (deflated 55%)\n  adding: quantized/model-int4/generation_config.json (deflated 29%)\n  adding: quantized/model-int4/model.safetensors (deflated 10%)\n  adding: quantized/QUANTISATION-REPORT.md (deflated 40%)\n  adding: merged_model/ (stored 0%)\n  adding: merged_model/config.json (deflated 48%)\n  adding: merged_model/tokenizer_config.json (deflated 69%)\n  adding: merged_model/tokenizer.json (deflated 85%)\n  adding: merged_model/special_tokens_map.json (deflated 79%)\n  adding: merged_model/chat_template.jinja (deflated 60%)\n  adding: merged_model/tokenizer.model (deflated 55%)\n  adding: merged_model/generation_config.json (deflated 29%)\n  adding: merged_model/model.safetensors (deflated 22%)\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}